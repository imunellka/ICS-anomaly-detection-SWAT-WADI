import torch
import torch.nn.functional as F
import numpy as np
import wandb
from tqdm.notebook import tqdm
from torch.utils.data import DataLoader
from typing import Optional, Tuple


def train_autoencoder(
    model: torch.nn.Module,
    train_loader: DataLoader,
    val_loader: Optional[DataLoader] = None,
    n_epochs: int = 50,
    lr: float = 1e-4,
    device: Optional[str] = None,
    use_wandb: bool = True,
    run_name: str = "autoencoder_run",
    project_name: str = "transformer_autoencoder"
) -> Tuple[list, list]:
    """
    –û–±—É—á–∞–µ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –∏ –ª–æ–≥–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.

    Args:
        model (torch.nn.Module): –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞.
        train_loader (DataLoader): –î–∞—Ç–∞–ª–æ–∞–¥–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        val_loader (DataLoader, optional): –î–∞—Ç–∞–ª–æ–∞–¥–µ—Ä –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
        n_epochs (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö.
        lr (float): Learning rate.
        device (str, optional): CUDA / CPU.
        use_wandb (bool): –í–∫–ª—é—á–∏—Ç—å wandb –ª–æ–≥–≥–∏–Ω–≥.
        run_name (str): –ò–º—è –∑–∞–ø—É—Å–∫–∞ –≤ wandb.
        project_name (str): –ò–º—è –ø—Ä–æ–µ–∫—Ç–∞ –≤ wandb.

    Returns:
        Tuple[List[float], List[float]]: –°–ø–∏—Å–∫–∏ –ø–æ—Ç–µ—Ä—å train/val –ø–æ —ç–ø–æ—Ö–∞–º.
    """

    if device is None:
        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

    model = model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    loss_fn = F.mse_loss

    if use_wandb:
        wandb.init(project=project_name, name=run_name)
        wandb.watch(model)

    train_losses = []
    val_losses = []

    for epoch in tqdm(range(n_epochs), desc="Training"):
        model.train()
        epoch_train_losses = []

        for X_batch in train_loader:
            x = X_batch[0].permute(0, 2, 1).to(device)  # (B, C, T)
            optimizer.zero_grad()
            reconstructed = model(x)
            loss = loss_fn(reconstructed, x)
            loss.backward()
            optimizer.step()
            epoch_train_losses.append(loss.item())

        train_loss = np.mean(epoch_train_losses)
        train_losses.append(train_loss)

        val_loss = None
        if val_loader:
            model.eval()
            epoch_val_losses = []
            with torch.no_grad():
                for X_batch in val_loader:
                    x = X_batch[0].permute(0, 2, 1).to(device)
                    reconstructed = model(x)
                    loss = loss_fn(reconstructed, x)
                    epoch_val_losses.append(loss.item())
            val_loss = np.mean(epoch_val_losses)
            val_losses.append(val_loss)

        if use_wandb:
            log_data = {"epoch": epoch + 1, "train_loss": train_loss}
            if val_loss is not None:
                log_data["val_loss"] = val_loss
            wandb.log(log_data)

        print(f"üì¶ Epoch [{epoch+1}/{n_epochs}] | Train Loss: {train_loss:.6f}" +
              (f" | Val Loss: {val_loss:.6f}" if val_loss is not None else ""))

    torch.save(model.state_dict(), f"{run_name}_weights.pth")
    print(f"‚úÖ Model saved as '{run_name}_weights.pth'")

    if use_wandb:
        wandb.finish()

    return train_losses, val_losses
